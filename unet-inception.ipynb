{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'task': 'Congestion',\n",
    "    'save_path': './UNetInception/',\n",
    "    'log_file': './UNetInception_log.txt',\n",
    "    'pretrained': None,\n",
    "    'max_iters': 100000,\n",
    "    'plot_roc': False,\n",
    "    'arg_file': None,\n",
    "    'cpu': False,\n",
    "    'dataroot': '/home/palaniappan-r/Repos/boosted-UNet/training_datasets/congestion_train_set/congestion',\n",
    "    'ann_file_train': './files/train.csv',\n",
    "    'ann_file_test': './files/test.csv',\n",
    "    'dataset_type': 'CongestionDataset',\n",
    "    'batch_size': 4,\n",
    "    'aug_pipeline': ['Flip'],\n",
    "    'model_type': 'Congestion_Prediction_Net',\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_type': 'MSELoss',\n",
    "    'eval_metric': ['NRMS', 'SSIM'],\n",
    "    'ann_file': './files/train.csv',\n",
    "    'test_mode': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mmcv\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "\n",
    "class Flip:\n",
    "    _directions = ['horizontal', 'vertical']\n",
    "\n",
    "    def __init__(self, keys=['feature', 'label'], flip_ratio=0.5, direction='horizontal', **kwargs):\n",
    "        if direction not in self._directions:\n",
    "            raise ValueError(f'Direction {direction} is not supported. Currently supported directions are {self._directions}')\n",
    "        self.keys = keys\n",
    "        self.flip_ratio = flip_ratio\n",
    "        self.direction = direction\n",
    "\n",
    "    def __call__(self, results):\n",
    "        if np.random.random() < self.flip_ratio:\n",
    "            for key in self.keys:\n",
    "                if isinstance(results[key], list):\n",
    "                    for v in results[key]:\n",
    "                        mmcv.imflip_(v, self.direction)\n",
    "                else:\n",
    "                    mmcv.imflip_(results[key], self.direction)\n",
    "        return results\n",
    "\n",
    "class Rotation:\n",
    "    def __init__(self, keys=['feature', 'label'], axis=(0, 1), rotate_ratio=0.5, **kwargs):\n",
    "        self.keys = keys\n",
    "        self.axis = {k: axis for k in keys} if isinstance(axis, tuple) else axis\n",
    "        self.rotate_ratio = rotate_ratio\n",
    "        self.directions = [0, -1, -2, -3]\n",
    "\n",
    "    def __call__(self, results):\n",
    "        if np.random.random() < self.rotate_ratio:\n",
    "            rotate_angle = self.directions[int(np.random.random() * 4)]\n",
    "            for key in self.keys:\n",
    "                if isinstance(results[key], list):\n",
    "                    for v in results[key]:\n",
    "                        v = np.ascontiguousarray(np.rot90(v, rotate_angle, axes=self.axis[key]))\n",
    "                else:\n",
    "                    results[key] = np.ascontiguousarray(np.rot90(results[key], rotate_angle, axes=self.axis[key]))\n",
    "        return results\n",
    "\n",
    "\n",
    "class IterLoader:\n",
    "    def __init__(self, dataloader):\n",
    "        self._dataloader = dataloader\n",
    "        self.iter_loader = iter(self._dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            data = next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            time.sleep(1)\n",
    "            self.iter_loader = iter(self._dataloader)\n",
    "            data = next(self.iter_loader)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_dataset(train_options):\n",
    "    aug_methods = {\n",
    "        'Flip': Flip(),\n",
    "        'Rotation': Rotation(**train_options)\n",
    "    }\n",
    "\n",
    "    if 'aug_pipeline' in train_options and not train_options['test_mode']:\n",
    "        pipeline = [aug_methods[method] for method in train_options.pop('aug_pipeline')]\n",
    "    else:\n",
    "        pipeline = None\n",
    "\n",
    "    dataset_cls = datasets.__dict__[train_options.pop('dataset_type')]\n",
    "    dataset = dataset_cls(**train_options, pipeline=pipeline)\n",
    "\n",
    "    if train_options['test_mode']:\n",
    "        return DataLoader(dataset=dataset, num_workers=1, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        return IterLoader(DataLoader(\n",
    "            dataset=dataset,\n",
    "            num_workers=1,\n",
    "            batch_size=train_options.pop('batch_size'),\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def generation_init_weights(module):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    module.apply(init_func)\n",
    "\n",
    "class CreateEncoderSingleConv(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, kernel):\n",
    "        super().__init__()\n",
    "        assert kernel % 2 == 1, \"Kernel size must be odd\"\n",
    "\n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, out_chs, kernel_size=kernel, padding=(kernel - 1) // 2),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.PReLU(num_parameters=out_chs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.single_conv(x)\n",
    "\n",
    "\n",
    "class EncoderInceptionModuleSingle(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        assert channels % 2 == 0, \"Channels must be even\"\n",
    "\n",
    "        bn_ch = channels // 2\n",
    "        self.bottleneck = CreateEncoderSingleConv(channels, bn_ch, 1)\n",
    "        self.conv1 = CreateEncoderSingleConv(bn_ch, channels, 1)\n",
    "        self.conv3 = CreateEncoderSingleConv(bn_ch, channels, 3)\n",
    "        self.conv5 = CreateEncoderSingleConv(bn_ch, channels, 5)\n",
    "        self.conv7 = CreateEncoderSingleConv(bn_ch, channels, 7)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(5, stride=1, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bn = self.bottleneck(x)\n",
    "        out = self.conv1(bn) + self.conv3(bn) + self.conv5(bn) + self.conv7(bn) + self.pool3(x) + self.pool5(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderModule(nn.Module):\n",
    "    def __init__(self, chs, repeat_num, use_inception):\n",
    "        super().__init__()\n",
    "        if use_inception:\n",
    "            layers = [EncoderInceptionModuleSingle(chs) for _ in range(repeat_num)]\n",
    "        else:\n",
    "            layers = [CreateEncoderSingleConv(chs, chs, 3) for _ in range(repeat_num)]\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "\n",
    "\n",
    "class IncepEncoder(nn.Module):\n",
    "    def __init__(self, use_inception, repeat_per_module, middle_layer_size=256):\n",
    "        super().__init__()\n",
    "        self.encoderPart = EncoderModule(middle_layer_size, repeat_per_module, use_inception)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoderPart(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        generation_init_weights(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU(num_parameters=out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.PReLU(num_parameters=out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64 )\n",
    "        self.down1 = Down(64 , 128)\n",
    "        self.down2 = Down(128 , 256 )\n",
    "        self.down3 = Down(256 , 512 )\n",
    "        self.down4 = Down(512 , 512 )\n",
    "        self.up1 = Up(1024 , 256 , bilinear)\n",
    "        self.up2 = Up(512 , 128 , bilinear)\n",
    "        self.up3 = Up(256 , 64 , bilinear)\n",
    "        self.up4 = Up(128 , 64 , bilinear)\n",
    "        self.outc = OutConv(64 , n_classes)\n",
    "\n",
    "        self.incepEncoder = IncepEncoder(True, 1, 512)\n",
    "        self.Conv4Inception = DoubleConv(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x5B = self.incepEncoder(x5)  # Insert the Inception Module at the bottleneck\n",
    "        x5C = self.Conv4Inception(x5B)\n",
    "        x = self.up1(x5C, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        generation_init_weights(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def generation_init_weights(module):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and ('Conv' in classname or 'Linear' in classname):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    module.apply(init_func)\n",
    "\n",
    "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "    unexpected_keys = []\n",
    "    all_missing_keys = []\n",
    "    err_msg = []\n",
    "\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     all_missing_keys, unexpected_keys, err_msg)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(module)\n",
    "    load = None\n",
    "\n",
    "    missing_keys = [key for key in all_missing_keys if 'num_batches_tracked' not in key]\n",
    "\n",
    "    if unexpected_keys:\n",
    "        err_msg.append(f'unexpected key in source state_dict: {\", \".join(unexpected_keys)}\\n')\n",
    "    if missing_keys:\n",
    "        err_msg.append(f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
    "\n",
    "    if err_msg:\n",
    "        err_msg.insert(0, 'The model and loaded state dict do not match exactly\\n')\n",
    "        err_msg = '\\n'.join(err_msg)\n",
    "        if strict:\n",
    "            raise RuntimeError(err_msg)\n",
    "        elif logger is not None:\n",
    "            logger.warning(err_msg)\n",
    "        else:\n",
    "            print(err_msg)\n",
    "    return missing_keys\n",
    "\n",
    "class PredictionNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        if isinstance(pretrained, str):\n",
    "            new_dict = OrderedDict()\n",
    "            weight = torch.load(pretrained, map_location='cpu')['state_dict']\n",
    "            for k, v in weight.items():\n",
    "                new_dict[k] = v\n",
    "            load_state_dict(self, new_dict, strict=strict, logger=None)\n",
    "        elif pretrained is None:\n",
    "            generation_init_weights(self)\n",
    "        else:\n",
    "            raise TypeError(\"'pretrained' must be a str or None.\")\n",
    "\n",
    "class Congestion_Prediction_Net(PredictionNet):\n",
    "    def __init__(self, in_channels=3, out_channels=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.uNet = UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.uNet(x)\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        super().init_weights(pretrained, strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "import functools\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['L1Loss', 'MSELoss']\n",
    "\n",
    "\n",
    "def build_loss(opt):\n",
    "    return globals()[opt.pop('loss_type')]()\n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    if reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "\n",
    "    return loss.sum()\n",
    "\n",
    "\n",
    "def mask_reduce_loss(loss, weight=None, reduction='mean', sample_wise=False):\n",
    "    if weight is not None:\n",
    "        assert weight.dim() == loss.dim()\n",
    "        assert weight.size(1) == 1 or weight.size(1) == loss.size(1)\n",
    "        loss = loss * weight\n",
    "\n",
    "    if weight is None or reduction == 'sum':\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    elif reduction == 'mean':\n",
    "        if weight.size(1) == 1:\n",
    "            weight = weight.expand_as(loss)\n",
    "        eps = 1e-12\n",
    "\n",
    "        if sample_wise:\n",
    "            weight = weight.sum(dim=[1, 2, 3], keepdim=True)\n",
    "            loss = (loss / (weight + eps)).sum() / weight.size(0)\n",
    "        else:\n",
    "            loss = loss.sum() / (weight.sum() + eps)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def masked_loss(loss_func):\n",
    "    @functools.wraps(loss_func)\n",
    "    def wrapper(pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                reduction='mean',\n",
    "                sample_wise=False,\n",
    "                **kwargs):\n",
    "        loss = loss_func(pred, target, **kwargs)\n",
    "        loss = mask_reduce_loss(loss, weight, reduction, sample_wise)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@masked_loss\n",
    "def l1_loss(pred, target):\n",
    "    return F.l1_loss(pred, target, reduction='none')\n",
    "\n",
    "\n",
    "@masked_loss\n",
    "def mse_loss(pred, target):\n",
    "    return F.mse_loss(pred, target, reduction='none')\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        self.sample_wise = sample_wise\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        return self.loss_weight * l1_loss(\n",
    "            pred,\n",
    "            target,\n",
    "            weight,\n",
    "            reduction=self.reduction,\n",
    "            sample_wise=self.sample_wise)\n",
    "\n",
    "\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
    "        super().__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        self.sample_wise = sample_wise\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        return self.loss_weight * mse_loss(\n",
    "            pred,\n",
    "            target,\n",
    "            weight,\n",
    "            reduction=self.reduction,\n",
    "            sample_wise=self.sample_wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = build_loss(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=model_config['lr'],\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=model_config['weight_decay']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import cos, pi\n",
    "from datetime import datetime\n",
    "class CosineRestartLr(object):\n",
    "    def __init__(self, base_lr, periods, restart_weights=[1], min_lr=None, min_lr_ratio=None):\n",
    "        self.periods = periods\n",
    "        self.min_lr = min_lr\n",
    "        self.min_lr_ratio = min_lr_ratio\n",
    "        self.restart_weights = restart_weights\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "        self.cumulative_periods = [\n",
    "            sum(self.periods[0:i + 1]) for i in range(len(self.periods))\n",
    "        ]\n",
    "\n",
    "    def annealing_cos(self, start: float, end: float, factor: float, weight: float = 1.) -> float:\n",
    "        cos_out = cos(pi * factor) + 1\n",
    "        return end + 0.5 * weight * (start - end) * cos_out\n",
    "\n",
    "    def get_position_from_periods(self, iteration: int, cumulative_periods):\n",
    "        for i, period in enumerate(cumulative_periods):\n",
    "            if iteration < period:\n",
    "                return i\n",
    "        raise ValueError(f'Current iteration {iteration} exceeds cumulative_periods {cumulative_periods}')\n",
    "\n",
    "    def get_lr(self, iter_num, base_lr: float):\n",
    "        target_lr = self.min_lr if self.min_lr is not None else base_lr\n",
    "\n",
    "        idx = self.get_position_from_periods(iter_num, self.cumulative_periods)\n",
    "        current_weight = self.restart_weights[idx]\n",
    "        nearest_restart = 0 if idx == 0 else self.cumulative_periods[idx - 1]\n",
    "        current_periods = self.periods[idx]\n",
    "\n",
    "        alpha = min((iter_num - nearest_restart) / current_periods, 1)\n",
    "        return self.annealing_cos(base_lr, target_lr, alpha, current_weight)\n",
    "\n",
    "    def _set_lr(self, optimizer, lr_groups):\n",
    "        if isinstance(optimizer, dict):\n",
    "            for k, optim in optimizer.items():\n",
    "                for param_group, lr in zip(optim.param_groups, lr_groups[k]):\n",
    "                    param_group['lr'] = lr\n",
    "        else:\n",
    "            for param_group, lr in zip(optimizer.param_groups, lr_groups):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "    def get_regular_lr(self, iter_num):\n",
    "        return [self.get_lr(iter_num, _base_lr) for _base_lr in self.base_lr]\n",
    "\n",
    "    def set_init_lr(self, optimizer):\n",
    "        for group in optimizer.param_groups:\n",
    "            group.setdefault('initial_lr', group['lr'])\n",
    "            self.base_lr = [group['initial_lr'] for group in optimizer.param_groups]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_lr = CosineRestartLr(model_config['lr'], [model_config['max_iters']], [1], 1e-7)\n",
    "cosine_lr.set_init_lr(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Iters(100/100000): Loss: 0.3691\n",
      "\n",
      "===> Iters(1000/100000): Loss: 0.2746\n",
      "\n",
      "===> Iters(2000/100000): Loss: 0.3077\n",
      "\n",
      "===> Iters(3000/100000): Loss: 0.2833\n",
      "\n",
      "===> Iters(4000/100000): Loss: 0.2604\n",
      "\n",
      "===> Iters(5000/100000): Loss: 0.2663\n",
      "\n",
      "===> Iters(6000/100000): Loss: 0.2679\n",
      "\n",
      "===> Iters(7000/100000): Loss: 0.2617\n",
      "\n",
      "===> Iters(8000/100000): Loss: 0.2614\n",
      "\n",
      "===> Iters(9000/100000): Loss: 0.2524\n",
      "\n",
      "===> Iters(10000/100000): Loss: 0.2483\n",
      "\n",
      "===> Iters(11000/100000): Loss: 0.2376\n",
      "\n",
      "===> Iters(12000/100000): Loss: 0.2621\n",
      "\n",
      "===> Iters(13000/100000): Loss: 0.2478\n",
      "\n",
      "===> Iters(14000/100000): Loss: 0.2319\n",
      "\n",
      "===> Iters(15000/100000): Loss: 0.2389\n",
      "\n",
      "===> Iters(16000/100000): Loss: 0.2441\n",
      "\n",
      "===> Iters(17000/100000): Loss: 0.2340\n",
      "\n",
      "===> Iters(18000/100000): Loss: 0.2308\n",
      "\n",
      "===> Iters(19000/100000): Loss: 0.2301\n",
      "\n",
      "===> Iters(20000/100000): Loss: 0.2370\n",
      "\n",
      "===> Iters(21000/100000): Loss: 0.2335\n",
      "\n",
      "===> Iters(22000/100000): Loss: 0.2284\n",
      "\n",
      "===> Iters(23000/100000): Loss: 0.2197\n",
      "\n",
      "===> Iters(24000/100000): Loss: 0.2216\n",
      "\n",
      "===> Iters(25000/100000): Loss: 0.2290\n",
      "\n",
      "===> Iters(26000/100000): Loss: 0.2217\n",
      "\n",
      "===> Iters(27000/100000): Loss: 0.2157\n",
      "\n",
      "===> Iters(28000/100000): Loss: 0.2197\n",
      "\n",
      "===> Iters(29000/100000): Loss: 0.2251\n",
      "\n",
      "===> Iters(30000/100000): Loss: 0.2261\n",
      "\n",
      "===> Iters(31000/100000): Loss: 0.2248\n",
      "\n",
      "===> Iters(32000/100000): Loss: 0.1914\n",
      "\n",
      "===> Iters(33000/100000): Loss: 0.2114\n",
      "\n",
      "===> Iters(34000/100000): Loss: 0.2063\n",
      "\n",
      "===> Iters(35000/100000): Loss: 0.2118\n",
      "\n",
      "===> Iters(36000/100000): Loss: 0.2120\n",
      "\n",
      "===> Iters(37000/100000): Loss: 0.1939\n",
      "\n",
      "===> Iters(38000/100000): Loss: 0.2083\n",
      "\n",
      "===> Iters(39000/100000): Loss: 0.2008\n",
      "\n",
      "===> Iters(40000/100000): Loss: 0.2058\n",
      "\n",
      "===> Iters(41000/100000): Loss: 0.1954\n",
      "\n",
      "===> Iters(42000/100000): Loss: 0.1919\n",
      "\n",
      "===> Iters(43000/100000): Loss: 0.2022\n",
      "\n",
      "===> Iters(44000/100000): Loss: 0.1831\n",
      "\n",
      "===> Iters(45000/100000): Loss: 0.2007\n",
      "\n",
      "===> Iters(46000/100000): Loss: 0.2053\n",
      "\n",
      "===> Iters(47000/100000): Loss: 0.2079\n",
      "\n",
      "===> Iters(48000/100000): Loss: 0.1890\n",
      "\n",
      "===> Iters(49000/100000): Loss: 0.1794\n",
      "\n",
      "===> Iters(50000/100000): Loss: 0.1875\n",
      "\n",
      "===> Iters(51000/100000): Loss: 0.1744\n",
      "\n",
      "===> Iters(52000/100000): Loss: 0.1760\n",
      "\n",
      "===> Iters(53000/100000): Loss: 0.1757\n",
      "\n",
      "===> Iters(54000/100000): Loss: 0.1872\n",
      "\n",
      "===> Iters(55000/100000): Loss: 0.1664\n",
      "\n",
      "===> Iters(56000/100000): Loss: 0.1666\n",
      "\n",
      "===> Iters(57000/100000): Loss: 0.1673\n",
      "\n",
      "===> Iters(58000/100000): Loss: 0.1656\n",
      "\n",
      "===> Iters(59000/100000): Loss: 0.1626\n",
      "\n",
      "===> Iters(60000/100000): Loss: 0.1677\n",
      "\n",
      "===> Iters(61000/100000): Loss: 0.1575\n",
      "\n",
      "===> Iters(62000/100000): Loss: 0.1634\n",
      "\n",
      "===> Iters(63000/100000): Loss: 0.1681\n",
      "\n",
      "===> Iters(64000/100000): Loss: 0.1572\n",
      "\n",
      "===> Iters(65000/100000): Loss: 0.1506\n",
      "\n",
      "===> Iters(66000/100000): Loss: 0.1607\n",
      "\n",
      "===> Iters(67000/100000): Loss: 0.1455\n",
      "\n",
      "===> Iters(68000/100000): Loss: 0.1545\n",
      "\n",
      "===> Iters(69000/100000): Loss: 0.1549\n",
      "\n",
      "===> Iters(70000/100000): Loss: 0.1496\n",
      "\n",
      "===> Iters(71000/100000): Loss: 0.1411\n",
      "\n",
      "===> Iters(72000/100000): Loss: 0.1537\n",
      "\n",
      "===> Iters(73000/100000): Loss: 0.1459\n",
      "\n",
      "===> Iters(74000/100000): Loss: 0.1469\n",
      "\n",
      "===> Iters(75000/100000): Loss: 0.1409\n",
      "\n",
      "===> Iters(76000/100000): Loss: 0.1455\n",
      "\n",
      "===> Iters(77000/100000): Loss: 0.1393\n",
      "\n",
      "===> Iters(78000/100000): Loss: 0.1441\n",
      "\n",
      "===> Iters(79000/100000): Loss: 0.1456\n",
      "\n",
      "===> Iters(80000/100000): Loss: 0.1413\n",
      "\n",
      "===> Iters(81000/100000): Loss: 0.1482\n",
      "\n",
      "===> Iters(82000/100000): Loss: 0.1388\n",
      "\n",
      "===> Iters(83000/100000): Loss: 0.1417\n",
      "\n",
      "===> Iters(84000/100000): Loss: 0.1369\n",
      "\n",
      "===> Iters(85000/100000): Loss: 0.1483\n",
      "\n",
      "===> Iters(86000/100000): Loss: 0.1408\n",
      "\n",
      "===> Iters(87000/100000): Loss: 0.1279\n",
      "\n",
      "===> Iters(88000/100000): Loss: 0.1369\n",
      "\n",
      "===> Iters(89000/100000): Loss: 0.1303\n",
      "\n",
      "===> Iters(90000/100000): Loss: 0.1319\n",
      "\n",
      "===> Iters(91000/100000): Loss: 0.1275\n",
      "\n",
      "===> Iters(92000/100000): Loss: 0.1342\n",
      "\n",
      "===> Iters(93000/100000): Loss: 0.1242\n",
      "\n",
      "===> Iters(94000/100000): Loss: 0.1399\n",
      "\n",
      "===> Iters(95000/100000): Loss: 0.1428\n",
      "\n",
      "===> Iters(96000/100000): Loss: 0.1380\n",
      "\n",
      "===> Iters(97000/100000): Loss: 0.1252\n",
      "\n",
      "===> Iters(98000/100000): Loss: 0.1385\n",
      "\n",
      "===> Iters(99000/100000): Loss: 0.1447\n",
      "\n",
      "===> Iters(100000/100000): Loss: 0.1279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "iter_num = 0\n",
    "print_freq = 100\n",
    "Show_freq = 1000\n",
    "\n",
    "lossList = []\n",
    "\n",
    "log_file = model_config['log_file']\n",
    "\n",
    "while iter_num < model_config['max_iters']:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    for feature, label, _ in dataset:\n",
    "        if model_config['cpu']:\n",
    "            input, target = feature, label\n",
    "        else:\n",
    "            input, target = feature.cuda(), label.cuda()\n",
    "\n",
    "        regular_lr = cosine_lr.get_regular_lr(iter_num)\n",
    "        cosine_lr._set_lr(optimizer, regular_lr)\n",
    "\n",
    "        prediction = model(input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pixel_loss = loss(prediction, target)\n",
    "        epoch_loss += pixel_loss.item()\n",
    "        pixel_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "        if iter_num % print_freq == 0 or iter_num == 100:\n",
    "            break\n",
    "\n",
    "    with open(log_file, 'a') as logs:\n",
    "        logs.write(\"===> Iters({}/{}): Loss: {:.4f}\".format(iter_num, model_config['max_iters'], epoch_loss / print_freq) + \"\\n\")\n",
    "\n",
    "    oneValue = epoch_loss / print_freq\n",
    "    lossList.append(oneValue)\n",
    "    if len(lossList) > 10:\n",
    "        lossList.pop(0)\n",
    "        sumValue = sum(lossList)\n",
    "\n",
    "    if iter_num % Show_freq == 0 or iter_num == 100:\n",
    "        print(\"===> Iters({}/{}): Loss: {:.4f}\".format(iter_num, model_config['max_iters'], epoch_loss / print_freq) + \"\\n\")\n",
    "\n",
    "    if iter_num % Show_freq == 0 or iter_num == 100:\n",
    "        later = datetime.now()\n",
    "        difference = (later - now).total_seconds()\n",
    "        difference_minutes = int(difference // 60)\n",
    "        difference_seconds = int(difference % 60)\n",
    "        max_iters = model_config['max_iters']\n",
    "        expect_difference = difference * (max_iters / iter_num)\n",
    "        difference_minutes = int(expect_difference // 60)\n",
    "        difference_seconds = int(expect_difference % 60)\n",
    "\n",
    "    epoch_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    'task': 'Congestion',\n",
    "    'save_path': './Congestion/',\n",
    "    'pretrained': None,\n",
    "    'max_iters': 100000,\n",
    "    'plot_roc': False,\n",
    "    'arg_file': None,\n",
    "    'cpu': False,\n",
    "    'dataroot': '/home/palaniappan-r/Repos/boosted-UNet/training_datasets/congestion_train_set/congestion',\n",
    "    'ann_file_train': './files/train.csv',\n",
    "    'ann_file_test': './files/test.csv',\n",
    "    'dataset_type': 'CongestionDataset',\n",
    "    'batch_size': 4,\n",
    "    'aug_pipeline': ['Flip'],\n",
    "    'model_type': 'Congestion_Prediction_Net',\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_type': 'MSELoss',\n",
    "    'eval_metric': ['NRMS', 'SSIM'],\n",
    "    'ann_file': './files/test.csv',\n",
    "    'test_mode': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from datasets.build_dataset import build_dataset\n",
    "from utils.metrics import build_metric, build_roc_prc_metric\n",
    "from utils.configs import Parser\n",
    "\n",
    "\n",
    "def inference(test_config, model):\n",
    "    # Build metrics\n",
    "    metrics = {k:build_metric(k) for k in test_config['eval_metric']}\n",
    "    avg_metrics = {k:0 for k in test_config['eval_metric']}\n",
    "\n",
    "    count =0\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for feature, label, label_path in dataset:\n",
    "            if test_config['cpu']:\n",
    "                input, target = feature, label\n",
    "            else:\n",
    "                input, target = feature.cuda(), label.cuda()\n",
    "\n",
    "            prediction = model(input)\n",
    "            for metric, metric_func in metrics.items():\n",
    "                if not metric_func(target.cpu(), prediction.squeeze(1).cpu()) == 1:\n",
    "                    avg_metrics[metric] += metric_func(target.cpu(), prediction.squeeze(1).cpu())\n",
    "\n",
    "            if test_config['plot_roc']:\n",
    "                save_path = osp.join(test_config['save_path'], 'test_result')\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                file_name = osp.splitext(osp.basename(label_path[0]))[0]\n",
    "                save_path = osp.join(save_path, f'{file_name}.npy')\n",
    "                output_final = prediction.float().detach().cpu().numpy()\n",
    "                np.save(save_path, output_final)\n",
    "                count +=1\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "\n",
    "    for metric, avg_metric in avg_metrics.items():\n",
    "        print(\"===> Avg. {}: {:.4f}\".format(metric, avg_metric / len(dataset))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3164/3164 [01:45<00:00, 30.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Avg. NRMS: 0.0483\n",
      "===> Avg. SSIM: 0.7871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference(test_config, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoutingCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
