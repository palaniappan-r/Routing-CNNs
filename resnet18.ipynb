{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'task': 'Congestion',\n",
    "    'save_path': './ResNet18/',\n",
    "    'log_file': './ResNet18_log.txt',\n",
    "    'pretrained': None,\n",
    "    'max_iters': 1000,\n",
    "    'plot_roc': False,\n",
    "    'arg_file': None,\n",
    "    'cpu': False,\n",
    "    'dataroot': '/home/palaniappan-r/Repos/boosted-UNet/training_datasets/congestion_train_set/congestion',\n",
    "    'ann_file_train': './files/train.csv',\n",
    "    'ann_file_test': './files/test.csv',\n",
    "    'dataset_type': 'CongestionDataset',\n",
    "    'batch_size': 4,\n",
    "    'aug_pipeline': ['Flip'],\n",
    "    'model_type': 'Congestion_Prediction_Net',\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_type': 'MSELoss',\n",
    "    'eval_metric': ['NRMS', 'SSIM'],\n",
    "    'ann_file': './files/train.csv',\n",
    "    'test_mode': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mmcv\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "\n",
    "class Flip:\n",
    "    _directions = ['horizontal', 'vertical']\n",
    "\n",
    "    def __init__(self, keys=['feature', 'label'], flip_ratio=0.5, direction='horizontal', **kwargs):\n",
    "        if direction not in self._directions:\n",
    "            raise ValueError(f'Direction {direction} is not supported. Currently supported directions are {self._directions}')\n",
    "        self.keys = keys\n",
    "        self.flip_ratio = flip_ratio\n",
    "        self.direction = direction\n",
    "\n",
    "    def __call__(self, results):\n",
    "        if np.random.random() < self.flip_ratio:\n",
    "            for key in self.keys:\n",
    "                if isinstance(results[key], list):\n",
    "                    for v in results[key]:\n",
    "                        mmcv.imflip_(v, self.direction)\n",
    "                else:\n",
    "                    mmcv.imflip_(results[key], self.direction)\n",
    "        return results\n",
    "\n",
    "class Rotation:\n",
    "    def __init__(self, keys=['feature', 'label'], axis=(0, 1), rotate_ratio=0.5, **kwargs):\n",
    "        self.keys = keys\n",
    "        self.axis = {k: axis for k in keys} if isinstance(axis, tuple) else axis\n",
    "        self.rotate_ratio = rotate_ratio\n",
    "        self.directions = [0, -1, -2, -3]\n",
    "\n",
    "    def __call__(self, results):\n",
    "        if np.random.random() < self.rotate_ratio:\n",
    "            rotate_angle = self.directions[int(np.random.random() * 4)]\n",
    "            for key in self.keys:\n",
    "                if isinstance(results[key], list):\n",
    "                    for v in results[key]:\n",
    "                        v = np.ascontiguousarray(np.rot90(v, rotate_angle, axes=self.axis[key]))\n",
    "                else:\n",
    "                    results[key] = np.ascontiguousarray(np.rot90(results[key], rotate_angle, axes=self.axis[key]))\n",
    "        return results\n",
    "\n",
    "\n",
    "class IterLoader:\n",
    "    def __init__(self, dataloader):\n",
    "        self._dataloader = dataloader\n",
    "        self.iter_loader = iter(self._dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            data = next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            time.sleep(1)\n",
    "            self.iter_loader = iter(self._dataloader)\n",
    "            data = next(self.iter_loader)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_dataset(train_options):\n",
    "    aug_methods = {\n",
    "        'Flip': Flip(),\n",
    "        'Rotation': Rotation(**train_options)\n",
    "    }\n",
    "\n",
    "    if 'aug_pipeline' in train_options and not train_options['test_mode']:\n",
    "        pipeline = [aug_methods[method] for method in train_options.pop('aug_pipeline')]\n",
    "    else:\n",
    "        pipeline = None\n",
    "\n",
    "    dataset_cls = datasets.__dict__[train_options.pop('dataset_type')]\n",
    "    dataset = dataset_cls(**train_options, pipeline=pipeline)\n",
    "\n",
    "    if train_options['test_mode']:\n",
    "        return DataLoader(dataset=dataset, num_workers=1, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        return IterLoader(DataLoader(\n",
    "            dataset=dataset,\n",
    "            num_workers=1,\n",
    "            batch_size=train_options.pop('batch_size'),\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def generation_init_weights(module):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and ('Conv' in classname or 'Linear' in classname):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    module.apply(init_func)\n",
    "\n",
    "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "    unexpected_keys = []\n",
    "    all_missing_keys = []\n",
    "    err_msg = []\n",
    "\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     all_missing_keys, unexpected_keys, err_msg)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(module)\n",
    "    load = None\n",
    "\n",
    "    missing_keys = [key for key in all_missing_keys if 'num_batches_tracked' not in key]\n",
    "\n",
    "    if unexpected_keys:\n",
    "        err_msg.append(f'unexpected key in source state_dict: {\", \".join(unexpected_keys)}\\n')\n",
    "    if missing_keys:\n",
    "        err_msg.append(f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
    "\n",
    "    if err_msg:\n",
    "        err_msg.insert(0, 'The model and loaded state dict do not match exactly\\n')\n",
    "        err_msg = '\\n'.join(err_msg)\n",
    "        if strict:\n",
    "            raise RuntimeError(err_msg)\n",
    "        elif logger is not None:\n",
    "            logger.warning(err_msg)\n",
    "        else:\n",
    "            print(err_msg)\n",
    "    return missing_keys\n",
    "\n",
    "class PredictionNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        if isinstance(pretrained, str):\n",
    "            new_dict = torch.load(pretrained, map_location='cpu')['state_dict']\n",
    "            load_state_dict(self, new_dict, strict=strict, logger=None)\n",
    "        elif pretrained is None:\n",
    "            generation_init_weights(self)\n",
    "        else:\n",
    "            raise TypeError(\"'pretrained' must be a str or None.\")\n",
    "\n",
    "class Congestion_Prediction_Net(PredictionNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.final_conv = nn.Conv2d(num_ftrs, 1, kernel_size=1)  # Output 1 channel\n",
    "        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)  # Upsample to 256x256\n",
    "        self.sigmoid = nn.Sigmoid()  # Use sigmoid to ensure output is in [0, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.final_conv(x.unsqueeze(2).unsqueeze(3)) \n",
    "\n",
    "        x = self.upsample(x) \n",
    "        x = self.sigmoid(x)  \n",
    "        return x\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        super().init_weights(pretrained, strict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congestion_Prediction_Net(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (final_conv): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (upsample): Upsample(size=(256, 256), mode='bilinear')\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/palaniappan-r/anaconda3/envs/RoutingCNN/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/palaniappan-r/anaconda3/envs/RoutingCNN/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = Congestion_Prediction_Net()\n",
    "model.init_weights()\n",
    "model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "import functools\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['L1Loss', 'MSELoss']\n",
    "\n",
    "\n",
    "def build_loss(opt):\n",
    "    return globals()[opt.pop('loss_type')]()\n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    if reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "\n",
    "    return loss.sum()\n",
    "\n",
    "\n",
    "def mask_reduce_loss(loss, weight=None, reduction='mean', sample_wise=False):\n",
    "    if weight is not None:\n",
    "        assert weight.dim() == loss.dim()\n",
    "        assert weight.size(1) == 1 or weight.size(1) == loss.size(1)\n",
    "        loss = loss * weight\n",
    "\n",
    "    if weight is None or reduction == 'sum':\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    elif reduction == 'mean':\n",
    "        if weight.size(1) == 1:\n",
    "            weight = weight.expand_as(loss)\n",
    "        eps = 1e-12\n",
    "\n",
    "        if sample_wise:\n",
    "            weight = weight.sum(dim=[1, 2, 3], keepdim=True)\n",
    "            loss = (loss / (weight + eps)).sum() / weight.size(0)\n",
    "        else:\n",
    "            loss = loss.sum() / (weight.sum() + eps)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def masked_loss(loss_func):\n",
    "    @functools.wraps(loss_func)\n",
    "    def wrapper(pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                reduction='mean',\n",
    "                sample_wise=False,\n",
    "                **kwargs):\n",
    "        loss = loss_func(pred, target, **kwargs)\n",
    "        loss = mask_reduce_loss(loss, weight, reduction, sample_wise)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@masked_loss\n",
    "def l1_loss(pred, target):\n",
    "    return F.l1_loss(pred, target, reduction='none')\n",
    "\n",
    "\n",
    "@masked_loss\n",
    "def mse_loss(pred, target):\n",
    "    return F.mse_loss(pred, target, reduction='none')\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        self.sample_wise = sample_wise\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        return self.loss_weight * l1_loss(\n",
    "            pred,\n",
    "            target,\n",
    "            weight,\n",
    "            reduction=self.reduction,\n",
    "            sample_wise=self.sample_wise)\n",
    "\n",
    "\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
    "        super().__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        self.sample_wise = sample_wise\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        return self.loss_weight * mse_loss(\n",
    "            pred,\n",
    "            target,\n",
    "            weight,\n",
    "            reduction=self.reduction,\n",
    "            sample_wise=self.sample_wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = build_loss(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=model_config['lr'],\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=model_config['weight_decay']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, pi\n",
    "from datetime import datetime\n",
    "class CosineRestartLr(object):\n",
    "    def __init__(self, base_lr, periods, restart_weights=[1], min_lr=None, min_lr_ratio=None):\n",
    "        self.periods = periods\n",
    "        self.min_lr = min_lr\n",
    "        self.min_lr_ratio = min_lr_ratio\n",
    "        self.restart_weights = restart_weights\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "        self.cumulative_periods = [\n",
    "            sum(self.periods[0:i + 1]) for i in range(len(self.periods))\n",
    "        ]\n",
    "\n",
    "    def annealing_cos(self, start: float, end: float, factor: float, weight: float = 1.) -> float:\n",
    "        cos_out = cos(pi * factor) + 1\n",
    "        return end + 0.5 * weight * (start - end) * cos_out\n",
    "\n",
    "    def get_position_from_periods(self, iteration: int, cumulative_periods):\n",
    "        for i, period in enumerate(cumulative_periods):\n",
    "            if iteration < period:\n",
    "                return i\n",
    "        raise ValueError(f'Current iteration {iteration} exceeds cumulative_periods {cumulative_periods}')\n",
    "\n",
    "    def get_lr(self, iter_num, base_lr: float):\n",
    "        target_lr = self.min_lr if self.min_lr is not None else base_lr\n",
    "\n",
    "        idx = self.get_position_from_periods(iter_num, self.cumulative_periods)\n",
    "        current_weight = self.restart_weights[idx]\n",
    "        nearest_restart = 0 if idx == 0 else self.cumulative_periods[idx - 1]\n",
    "        current_periods = self.periods[idx]\n",
    "\n",
    "        alpha = min((iter_num - nearest_restart) / current_periods, 1)\n",
    "        return self.annealing_cos(base_lr, target_lr, alpha, current_weight)\n",
    "\n",
    "    def _set_lr(self, optimizer, lr_groups):\n",
    "        if isinstance(optimizer, dict):\n",
    "            for k, optim in optimizer.items():\n",
    "                for param_group, lr in zip(optim.param_groups, lr_groups[k]):\n",
    "                    param_group['lr'] = lr\n",
    "        else:\n",
    "            for param_group, lr in zip(optimizer.param_groups, lr_groups):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "    def get_regular_lr(self, iter_num):\n",
    "        return [self.get_lr(iter_num, _base_lr) for _base_lr in self.base_lr]\n",
    "\n",
    "    def set_init_lr(self, optimizer):\n",
    "        for group in optimizer.param_groups:\n",
    "            group.setdefault('initial_lr', group['lr'])\n",
    "            self.base_lr = [group['initial_lr'] for group in optimizer.param_groups]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_lr = CosineRestartLr(model_config['lr'], [model_config['max_iters']], [1], 1e-7)\n",
    "cosine_lr.set_init_lr(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    'task': 'Congestion',\n",
    "    'save_path': './Congestion/',\n",
    "    'pretrained': None,\n",
    "    'max_iters': 200000,\n",
    "    'plot_roc': False,\n",
    "    'arg_file': None,\n",
    "    'cpu': False,\n",
    "    'dataroot': '/home/palaniappan-r/Repos/boosted-UNet/training_datasets/congestion_train_set/congestion',\n",
    "    'ann_file_train': './files/train.csv',\n",
    "    'ann_file_test': './files/test.csv',\n",
    "    'dataset_type': 'CongestionDataset',\n",
    "    'batch_size': 4,\n",
    "    'aug_pipeline': ['Flip'],\n",
    "    'model_type': 'Congestion_Prediction_Net',\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_type': 'MSELoss',\n",
    "    'eval_metric': ['NRMS', 'SSIM'],\n",
    "    'ann_file': './files/test.csv',\n",
    "    'test_mode': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(test_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.metrics import build_metric\n",
    "\n",
    "\n",
    "def inference(test_config, model):\n",
    "    # Build metrics\n",
    "    metrics = {k:build_metric(k) for k in test_config['eval_metric']}\n",
    "    avg_metrics = {k:0 for k in test_config['eval_metric']}\n",
    "\n",
    "    count =0\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for feature, label, label_path in dataset:\n",
    "            if test_config['cpu']:\n",
    "                input, target = feature, label\n",
    "            else:\n",
    "                input, target = feature.cuda(), label.cuda()\n",
    "\n",
    "            prediction = model(input)\n",
    "            for metric, metric_func in metrics.items():\n",
    "                if not metric_func(target.cpu(), prediction.squeeze(1).cpu()) == 1:\n",
    "                    avg_metrics[metric] += metric_func(target.cpu(), prediction.squeeze(1).cpu())\n",
    "\n",
    "            if test_config['plot_roc']:\n",
    "                save_path = osp.join(test_config['save_path'], 'test_result')\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                file_name = osp.splitext(osp.basename(label_path[0]))[0]\n",
    "                save_path = osp.join(save_path, f'{file_name}.npy')\n",
    "                output_final = prediction.float().detach().cpu().numpy()\n",
    "                np.save(save_path, output_final)\n",
    "                count +=1\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "\n",
    "    for metric, avg_metric in avg_metrics.items():\n",
    "        print(\"===> Avg. {}: {:.4f}\".format(metric, avg_metric / len(dataset))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.metrics import build_metric\n",
    "\n",
    "\n",
    "def inference(test_config, model):\n",
    "    # Build metrics\n",
    "    metrics = {k:build_metric(k) for k in test_config['eval_metric']}\n",
    "    avg_metrics = {k:0 for k in test_config['eval_metric']}\n",
    "\n",
    "    count =0\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for feature, label, label_path in dataset:\n",
    "            if test_config['cpu']:\n",
    "                input, target = feature, label\n",
    "            else:\n",
    "                input, target = feature.cuda(), label.cuda()\n",
    "\n",
    "            prediction = model(input)\n",
    "            for metric, metric_func in metrics.items():\n",
    "                if not metric_func(target.cpu(), prediction.squeeze(1).cpu()) == 1:\n",
    "                    avg_metrics[metric] += metric_func(target.cpu(), prediction.squeeze(1).cpu())\n",
    "\n",
    "            if test_config['plot_roc']:\n",
    "                save_path = osp.join(test_config['save_path'], 'test_result')\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                file_name = osp.splitext(osp.basename(label_path[0]))[0]\n",
    "                save_path = osp.join(save_path, f'{file_name}.npy')\n",
    "                output_final = prediction.float().detach().cpu().numpy()\n",
    "                np.save(save_path, output_final)\n",
    "                count +=1\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "\n",
    "    for metric, avg_metric in avg_metrics.items():\n",
    "        print(\"===> Avg. {}: {:.4f}\".format(metric, avg_metric / len(dataset))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3164/3164 [00:55<00:00, 56.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Avg. NRMS: 0.0586\n",
      "===> Avg. SSIM: 0.7676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference(test_config, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoutingCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
