{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'task': 'Congestion',\n",
    "    'save_path': './RouteNet/',\n",
    "    'log_file': './RouteNet_log.txt',\n",
    "    'pretrained': None,\n",
    "    'max_iters': 100000,\n",
    "    'plot_roc': False,\n",
    "    'arg_file': None,\n",
    "    'cpu': False,\n",
    "    'dataroot': '/home/palaniappan-r/Repos/boosted-UNet/training_datasets/congestion_train_set/congestion',\n",
    "    'ann_file_train': './files/train.csv',\n",
    "    'ann_file_test': './files/test.csv',\n",
    "    'dataset_type': 'CongestionDataset',\n",
    "    'batch_size': 4,\n",
    "    'aug_pipeline': ['Flip'],\n",
    "    'model_type': 'Congestion_Prediction_Net',\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_type': 'MSELoss',\n",
    "    'eval_metric': ['NRMS', 'SSIM'],\n",
    "    'ann_file': './files/train.csv',\n",
    "    'test_mode': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import mmcv\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "\n",
    "class Flip:\n",
    "    _directions = ['horizontal', 'vertical']\n",
    "\n",
    "    def __init__(self, keys=['feature', 'label'], flip_ratio=0.5, direction='horizontal', **kwargs):\n",
    "        if direction not in self._directions:\n",
    "            raise ValueError(f'Direction {direction} is not supported. Currently supported directions are {self._directions}')\n",
    "        self.keys = keys\n",
    "        self.flip_ratio = flip_ratio\n",
    "        self.direction = direction\n",
    "\n",
    "    def __call__(self, results):\n",
    "        if np.random.random() < self.flip_ratio:\n",
    "            for key in self.keys:\n",
    "                if isinstance(results[key], list):\n",
    "                    for v in results[key]:\n",
    "                        mmcv.imflip_(v, self.direction)\n",
    "                else:\n",
    "                    mmcv.imflip_(results[key], self.direction)\n",
    "        return results\n",
    "\n",
    "class Rotation:\n",
    "    def __init__(self, keys=['feature', 'label'], axis=(0, 1), rotate_ratio=0.5, **kwargs):\n",
    "        self.keys = keys\n",
    "        self.axis = {k: axis for k in keys} if isinstance(axis, tuple) else axis\n",
    "        self.rotate_ratio = rotate_ratio\n",
    "        self.directions = [0, -1, -2, -3]\n",
    "\n",
    "    def __call__(self, results):\n",
    "        if np.random.random() < self.rotate_ratio:\n",
    "            rotate_angle = self.directions[int(np.random.random() * 4)]\n",
    "            for key in self.keys:\n",
    "                if isinstance(results[key], list):\n",
    "                    for v in results[key]:\n",
    "                        v = np.ascontiguousarray(np.rot90(v, rotate_angle, axes=self.axis[key]))\n",
    "                else:\n",
    "                    results[key] = np.ascontiguousarray(np.rot90(results[key], rotate_angle, axes=self.axis[key]))\n",
    "        return results\n",
    "\n",
    "\n",
    "class IterLoader:\n",
    "    def __init__(self, dataloader):\n",
    "        self._dataloader = dataloader\n",
    "        self.iter_loader = iter(self._dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            data = next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            time.sleep(1)\n",
    "            self.iter_loader = iter(self._dataloader)\n",
    "            data = next(self.iter_loader)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "def build_dataset(train_options):\n",
    "    aug_methods = {\n",
    "        'Flip': Flip(),\n",
    "        'Rotation': Rotation(**train_options)\n",
    "    }\n",
    "\n",
    "    if 'aug_pipeline' in train_options and not train_options['test_mode']:\n",
    "        pipeline = [aug_methods[method] for method in train_options.pop('aug_pipeline')]\n",
    "    else:\n",
    "        pipeline = None\n",
    "\n",
    "    dataset_cls = datasets.__dict__[train_options.pop('dataset_type')]\n",
    "    dataset = dataset_cls(**train_options, pipeline=pipeline)\n",
    "\n",
    "    if train_options['test_mode']:\n",
    "        return DataLoader(dataset=dataset, num_workers=1, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        return IterLoader(DataLoader(\n",
    "            dataset=dataset,\n",
    "            num_workers=1,\n",
    "            batch_size=train_options.pop('batch_size'),\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def generation_init_weights(module):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    module.apply(init_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def generation_init_weights(module):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1\n",
    "                                    or classname.find('Linear') != -1):\n",
    "            \n",
    "            if hasattr(m, 'weight') and m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    module.apply(init_func)\n",
    "\n",
    "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "    unexpected_keys = []\n",
    "    all_missing_keys = []\n",
    "    err_msg = []\n",
    "\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     all_missing_keys, unexpected_keys,\n",
    "                                     err_msg)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(module)\n",
    "    load = None\n",
    "\n",
    "    missing_keys = [\n",
    "        key for key in all_missing_keys if 'num_batches_tracked' not in key\n",
    "    ]\n",
    "\n",
    "    if unexpected_keys:\n",
    "        err_msg.append('unexpected key in source '\n",
    "                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n",
    "    if missing_keys:\n",
    "        err_msg.append(\n",
    "            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
    "\n",
    "    if len(err_msg) > 0:\n",
    "        err_msg.insert(\n",
    "            0, 'The model and loaded state dict do not match exactly\\n')\n",
    "        err_msg = '\\n'.join(err_msg)\n",
    "        if strict:\n",
    "            raise RuntimeError(err_msg)\n",
    "        elif logger is not None:\n",
    "            logger.warning(err_msg)\n",
    "        else:\n",
    "            print(err_msg)\n",
    "    return missing_keys\n",
    "\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "        super(conv, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "            nn.BatchNorm2d(dim_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
    "            nn.BatchNorm2d(dim_out),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class upconv(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(upconv, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "                nn.ConvTranspose2d(dim_in, dim_out, 4, 2, 1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, out_dim=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.c1 = conv(in_dim, 32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)  # 这里先用这个\n",
    "        self.c2 = conv(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)  # 这里先用这个\n",
    "        self.c3 = nn.Sequential(\n",
    "                nn.Conv2d(64, out_dim, 3, 1, 1),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        generation_init_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h1 = self.c1(input)\n",
    "        h2 = self.pool1(h1)\n",
    "        h3 = self.c2(h2)\n",
    "        h4 = self.pool2(h3)\n",
    "        h5 = self.c3(h4)\n",
    "        return h5, h2\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_dim=2, in_dim=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = conv(in_dim, 32)\n",
    "        self.upc1 = upconv(32, 16)\n",
    "        self.conv2 = conv(16, 16)\n",
    "        self.upc2 = upconv(32+16, 4)\n",
    "        self.conv3 =  nn.Sequential(\n",
    "                nn.Conv2d(4, out_dim, 3, 1, 1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "    def init_weights(self):\n",
    "        generation_init_weights(self)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        feature, skip = input\n",
    "        d1 = self.conv1(feature)\n",
    "        d2 = self.upc1(d1)\n",
    "        d3 = self.conv2(d2)\n",
    "        d4 = self.upc2(torch.cat([d3, skip], dim=1))\n",
    "        output = self.conv3(d4)  # shortpath from 2->7\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class RouteNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_dim=in_channels)\n",
    "        self.decoder = Decoder(out_dim=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        if isinstance(pretrained, str):\n",
    "            new_dict = OrderedDict()\n",
    "            weight = torch.load(pretrained, map_location='cpu')['state_dict']\n",
    "            for k in weight.keys():\n",
    "                new_dict[k] = weight[k]\n",
    "            load_state_dict(self, new_dict, strict=strict, logger=None)\n",
    "        elif pretrained is None:\n",
    "            self.encoder.init_weights()\n",
    "            self.decoder.init_weights()\n",
    "        else:\n",
    "            raise TypeError(\"'pretrained' must be a str or None. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def generation_init_weights(module):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and ('Conv' in classname or 'Linear' in classname):\n",
    "            if m.weight is not None:\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    module.apply(init_func)\n",
    "\n",
    "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
    "    unexpected_keys = []\n",
    "    all_missing_keys = []\n",
    "    err_msg = []\n",
    "\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     all_missing_keys, unexpected_keys, err_msg)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "\n",
    "    load(module)\n",
    "    load = None\n",
    "\n",
    "    missing_keys = [key for key in all_missing_keys if 'num_batches_tracked' not in key]\n",
    "\n",
    "    if unexpected_keys:\n",
    "        err_msg.append(f'unexpected key in source state_dict: {\", \".join(unexpected_keys)}\\n')\n",
    "    if missing_keys:\n",
    "        err_msg.append(f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
    "\n",
    "    if err_msg:\n",
    "        err_msg.insert(0, 'The model and loaded state dict do not match exactly\\n')\n",
    "        err_msg = '\\n'.join(err_msg)\n",
    "        if strict:\n",
    "            raise RuntimeError(err_msg)\n",
    "        elif logger is not None:\n",
    "            logger.warning(err_msg)\n",
    "        else:\n",
    "            print(err_msg)\n",
    "    return missing_keys\n",
    "\n",
    "class PredictionNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        if isinstance(pretrained, str):\n",
    "            new_dict = OrderedDict()\n",
    "            weight = torch.load(pretrained, map_location='cpu')['state_dict']\n",
    "            for k, v in weight.items():\n",
    "                new_dict[k] = v\n",
    "            load_state_dict(self, new_dict, strict=strict, logger=None)\n",
    "        elif pretrained is None:\n",
    "            generation_init_weights(self)\n",
    "        else:\n",
    "            raise TypeError(\"'pretrained' must be a str or None.\")\n",
    "\n",
    "class Congestion_Prediction_Net(PredictionNet):\n",
    "    def __init__(self, in_channels=3, out_channels=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.uNet = RouteNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.uNet(x)\n",
    "\n",
    "    def init_weights(self, pretrained=None, strict=True, **kwargs):\n",
    "        super().init_weights(pretrained, strict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congestion_Prediction_Net(\n",
      "  (uNet): RouteNet(\n",
      "    (encoder): Encoder(\n",
      "      (c1): conv(\n",
      "        (main): Sequential(\n",
      "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (c2): conv(\n",
      "        (main): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (c3): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (conv1): conv(\n",
      "        (main): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (upc1): upconv(\n",
      "        (main): Sequential(\n",
      "          (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (conv2): conv(\n",
      "        (main): Sequential(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (upc2): upconv(\n",
      "        (main): Sequential(\n",
      "          (0): ConvTranspose2d(48, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Congestion_Prediction_Net()\n",
    "model.init_weights()\n",
    "model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "import functools\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['L1Loss', 'MSELoss']\n",
    "\n",
    "\n",
    "def build_loss(opt):\n",
    "    return globals()[opt.pop('loss_type')]()\n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    if reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "\n",
    "    return loss.sum()\n",
    "\n",
    "\n",
    "def mask_reduce_loss(loss, weight=None, reduction='mean', sample_wise=False):\n",
    "    if weight is not None:\n",
    "        assert weight.dim() == loss.dim()\n",
    "        assert weight.size(1) == 1 or weight.size(1) == loss.size(1)\n",
    "        loss = loss * weight\n",
    "\n",
    "    if weight is None or reduction == 'sum':\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    elif reduction == 'mean':\n",
    "        if weight.size(1) == 1:\n",
    "            weight = weight.expand_as(loss)\n",
    "        eps = 1e-12\n",
    "\n",
    "        if sample_wise:\n",
    "            weight = weight.sum(dim=[1, 2, 3], keepdim=True)\n",
    "            loss = (loss / (weight + eps)).sum() / weight.size(0)\n",
    "        else:\n",
    "            loss = loss.sum() / (weight.sum() + eps)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def masked_loss(loss_func):\n",
    "    @functools.wraps(loss_func)\n",
    "    def wrapper(pred,\n",
    "                target,\n",
    "                weight=None,\n",
    "                reduction='mean',\n",
    "                sample_wise=False,\n",
    "                **kwargs):\n",
    "        loss = loss_func(pred, target, **kwargs)\n",
    "        loss = mask_reduce_loss(loss, weight, reduction, sample_wise)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@masked_loss\n",
    "def l1_loss(pred, target):\n",
    "    return F.l1_loss(pred, target, reduction='none')\n",
    "\n",
    "\n",
    "@masked_loss\n",
    "def mse_loss(pred, target):\n",
    "    return F.mse_loss(pred, target, reduction='none')\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        self.sample_wise = sample_wise\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        return self.loss_weight * l1_loss(\n",
    "            pred,\n",
    "            target,\n",
    "            weight,\n",
    "            reduction=self.reduction,\n",
    "            sample_wise=self.sample_wise)\n",
    "\n",
    "\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
    "        super().__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "        self.sample_wise = sample_wise\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        return self.loss_weight * mse_loss(\n",
    "            pred,\n",
    "            target,\n",
    "            weight,\n",
    "            reduction=self.reduction,\n",
    "            sample_wise=self.sample_wise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = build_loss(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=model_config['lr'],\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=model_config['weight_decay']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import cos, pi\n",
    "from datetime import datetime\n",
    "class CosineRestartLr(object):\n",
    "    def __init__(self, base_lr, periods, restart_weights=[1], min_lr=None, min_lr_ratio=None):\n",
    "        self.periods = periods\n",
    "        self.min_lr = min_lr\n",
    "        self.min_lr_ratio = min_lr_ratio\n",
    "        self.restart_weights = restart_weights\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "        self.cumulative_periods = [\n",
    "            sum(self.periods[0:i + 1]) for i in range(len(self.periods))\n",
    "        ]\n",
    "\n",
    "    def annealing_cos(self, start: float, end: float, factor: float, weight: float = 1.) -> float:\n",
    "        cos_out = cos(pi * factor) + 1\n",
    "        return end + 0.5 * weight * (start - end) * cos_out\n",
    "\n",
    "    def get_position_from_periods(self, iteration: int, cumulative_periods):\n",
    "        for i, period in enumerate(cumulative_periods):\n",
    "            if iteration < period:\n",
    "                return i\n",
    "        raise ValueError(f'Current iteration {iteration} exceeds cumulative_periods {cumulative_periods}')\n",
    "\n",
    "    def get_lr(self, iter_num, base_lr: float):\n",
    "        target_lr = self.min_lr if self.min_lr is not None else base_lr\n",
    "\n",
    "        idx = self.get_position_from_periods(iter_num, self.cumulative_periods)\n",
    "        current_weight = self.restart_weights[idx]\n",
    "        nearest_restart = 0 if idx == 0 else self.cumulative_periods[idx - 1]\n",
    "        current_periods = self.periods[idx]\n",
    "\n",
    "        alpha = min((iter_num - nearest_restart) / current_periods, 1)\n",
    "        return self.annealing_cos(base_lr, target_lr, alpha, current_weight)\n",
    "\n",
    "    def _set_lr(self, optimizer, lr_groups):\n",
    "        if isinstance(optimizer, dict):\n",
    "            for k, optim in optimizer.items():\n",
    "                for param_group, lr in zip(optim.param_groups, lr_groups[k]):\n",
    "                    param_group['lr'] = lr\n",
    "        else:\n",
    "            for param_group, lr in zip(optimizer.param_groups, lr_groups):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "    def get_regular_lr(self, iter_num):\n",
    "        return [self.get_lr(iter_num, _base_lr) for _base_lr in self.base_lr]\n",
    "\n",
    "    def set_init_lr(self, optimizer):\n",
    "        for group in optimizer.param_groups:\n",
    "            group.setdefault('initial_lr', group['lr'])\n",
    "            self.base_lr = [group['initial_lr'] for group in optimizer.param_groups]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_lr = CosineRestartLr(model_config['lr'], [model_config['max_iters']], [1], 1e-7)\n",
    "cosine_lr.set_init_lr(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Iters(100/100000): Loss: 10.7554\n",
      "\n",
      "===> Iters(1000/100000): Loss: 0.3593\n",
      "\n",
      "===> Iters(2000/100000): Loss: 0.2786\n",
      "\n",
      "===> Iters(3000/100000): Loss: 0.2707\n",
      "\n",
      "===> Iters(4000/100000): Loss: 0.2903\n",
      "\n",
      "===> Iters(5000/100000): Loss: 0.2810\n",
      "\n",
      "===> Iters(6000/100000): Loss: 0.2675\n",
      "\n",
      "===> Iters(7000/100000): Loss: 0.2748\n",
      "\n",
      "===> Iters(8000/100000): Loss: 0.2717\n",
      "\n",
      "===> Iters(9000/100000): Loss: 0.2650\n",
      "\n",
      "===> Iters(10000/100000): Loss: 0.2663\n",
      "\n",
      "===> Iters(11000/100000): Loss: 0.2579\n",
      "\n",
      "===> Iters(12000/100000): Loss: 0.2620\n",
      "\n",
      "===> Iters(13000/100000): Loss: 0.2906\n",
      "\n",
      "===> Iters(14000/100000): Loss: 0.2608\n",
      "\n",
      "===> Iters(15000/100000): Loss: 0.2684\n",
      "\n",
      "===> Iters(16000/100000): Loss: 0.2650\n",
      "\n",
      "===> Iters(17000/100000): Loss: 0.2605\n",
      "\n",
      "===> Iters(18000/100000): Loss: 0.2674\n",
      "\n",
      "===> Iters(19000/100000): Loss: 0.2488\n",
      "\n",
      "===> Iters(20000/100000): Loss: 0.2650\n",
      "\n",
      "===> Iters(21000/100000): Loss: 0.2432\n",
      "\n",
      "===> Iters(22000/100000): Loss: 0.2471\n",
      "\n",
      "===> Iters(23000/100000): Loss: 0.2455\n",
      "\n",
      "===> Iters(24000/100000): Loss: 0.2684\n",
      "\n",
      "===> Iters(25000/100000): Loss: 0.2494\n",
      "\n",
      "===> Iters(26000/100000): Loss: 0.2766\n",
      "\n",
      "===> Iters(27000/100000): Loss: 0.2777\n",
      "\n",
      "===> Iters(28000/100000): Loss: 0.2614\n",
      "\n",
      "===> Iters(29000/100000): Loss: 0.2530\n",
      "\n",
      "===> Iters(30000/100000): Loss: 0.2583\n",
      "\n",
      "===> Iters(31000/100000): Loss: 0.2594\n",
      "\n",
      "===> Iters(32000/100000): Loss: 0.2616\n",
      "\n",
      "===> Iters(33000/100000): Loss: 0.2641\n",
      "\n",
      "===> Iters(34000/100000): Loss: 0.2425\n",
      "\n",
      "===> Iters(35000/100000): Loss: 0.2748\n",
      "\n",
      "===> Iters(36000/100000): Loss: 0.2548\n",
      "\n",
      "===> Iters(37000/100000): Loss: 0.2522\n",
      "\n",
      "===> Iters(38000/100000): Loss: 0.2376\n",
      "\n",
      "===> Iters(39000/100000): Loss: 0.2542\n",
      "\n",
      "===> Iters(40000/100000): Loss: 0.2574\n",
      "\n",
      "===> Iters(41000/100000): Loss: 0.2677\n",
      "\n",
      "===> Iters(42000/100000): Loss: 0.2504\n",
      "\n",
      "===> Iters(43000/100000): Loss: 0.2673\n",
      "\n",
      "===> Iters(44000/100000): Loss: 0.2596\n",
      "\n",
      "===> Iters(45000/100000): Loss: 0.2288\n",
      "\n",
      "===> Iters(46000/100000): Loss: 0.2617\n",
      "\n",
      "===> Iters(47000/100000): Loss: 0.2480\n",
      "\n",
      "===> Iters(48000/100000): Loss: 0.2531\n",
      "\n",
      "===> Iters(49000/100000): Loss: 0.2524\n",
      "\n",
      "===> Iters(50000/100000): Loss: 0.2471\n",
      "\n",
      "===> Iters(51000/100000): Loss: 0.2479\n",
      "\n",
      "===> Iters(52000/100000): Loss: 0.2590\n",
      "\n",
      "===> Iters(53000/100000): Loss: 0.2680\n",
      "\n",
      "===> Iters(54000/100000): Loss: 0.2539\n",
      "\n",
      "===> Iters(55000/100000): Loss: 0.2559\n",
      "\n",
      "===> Iters(56000/100000): Loss: 0.2500\n",
      "\n",
      "===> Iters(57000/100000): Loss: 0.2758\n",
      "\n",
      "===> Iters(58000/100000): Loss: 0.2403\n",
      "\n",
      "===> Iters(59000/100000): Loss: 0.2421\n",
      "\n",
      "===> Iters(60000/100000): Loss: 0.2611\n",
      "\n",
      "===> Iters(61000/100000): Loss: 0.2649\n",
      "\n",
      "===> Iters(62000/100000): Loss: 0.2478\n",
      "\n",
      "===> Iters(63000/100000): Loss: 0.2668\n",
      "\n",
      "===> Iters(64000/100000): Loss: 0.2432\n",
      "\n",
      "===> Iters(65000/100000): Loss: 0.2626\n",
      "\n",
      "===> Iters(66000/100000): Loss: 0.2506\n",
      "\n",
      "===> Iters(67000/100000): Loss: 0.2456\n",
      "\n",
      "===> Iters(68000/100000): Loss: 0.2580\n",
      "\n",
      "===> Iters(69000/100000): Loss: 0.2551\n",
      "\n",
      "===> Iters(70000/100000): Loss: 0.2677\n",
      "\n",
      "===> Iters(71000/100000): Loss: 0.2438\n",
      "\n",
      "===> Iters(72000/100000): Loss: 0.2505\n",
      "\n",
      "===> Iters(73000/100000): Loss: 0.2382\n",
      "\n",
      "===> Iters(74000/100000): Loss: 0.2579\n",
      "\n",
      "===> Iters(75000/100000): Loss: 0.2460\n",
      "\n",
      "===> Iters(76000/100000): Loss: 0.2544\n",
      "\n",
      "===> Iters(77000/100000): Loss: 0.2359\n",
      "\n",
      "===> Iters(78000/100000): Loss: 0.2358\n",
      "\n",
      "===> Iters(79000/100000): Loss: 0.2393\n",
      "\n",
      "===> Iters(80000/100000): Loss: 0.2514\n",
      "\n",
      "===> Iters(81000/100000): Loss: 0.2440\n",
      "\n",
      "===> Iters(82000/100000): Loss: 0.2472\n",
      "\n",
      "===> Iters(83000/100000): Loss: 0.2449\n",
      "\n",
      "===> Iters(84000/100000): Loss: 0.2244\n",
      "\n",
      "===> Iters(85000/100000): Loss: 0.2599\n",
      "\n",
      "===> Iters(86000/100000): Loss: 0.2427\n",
      "\n",
      "===> Iters(87000/100000): Loss: 0.2361\n",
      "\n",
      "===> Iters(88000/100000): Loss: 0.2561\n",
      "\n",
      "===> Iters(89000/100000): Loss: 0.2383\n",
      "\n",
      "===> Iters(90000/100000): Loss: 0.2376\n",
      "\n",
      "===> Iters(91000/100000): Loss: 0.2491\n",
      "\n",
      "===> Iters(92000/100000): Loss: 0.2366\n",
      "\n",
      "===> Iters(93000/100000): Loss: 0.2427\n",
      "\n",
      "===> Iters(94000/100000): Loss: 0.2390\n",
      "\n",
      "===> Iters(95000/100000): Loss: 0.2502\n",
      "\n",
      "===> Iters(96000/100000): Loss: 0.2324\n",
      "\n",
      "===> Iters(97000/100000): Loss: 0.2554\n",
      "\n",
      "===> Iters(98000/100000): Loss: 0.2496\n",
      "\n",
      "===> Iters(99000/100000): Loss: 0.2432\n",
      "\n",
      "===> Iters(100000/100000): Loss: 0.2486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "iter_num = 0\n",
    "print_freq = 100\n",
    "Show_freq = 1000\n",
    "\n",
    "lossList = []\n",
    "\n",
    "log_file = model_config['log_file']\n",
    "\n",
    "while iter_num < model_config['max_iters']:\n",
    "    now = datetime.now()\n",
    "    \n",
    "    for feature, label, _ in dataset:\n",
    "        if model_config['cpu']:\n",
    "            input, target = feature, label\n",
    "        else:\n",
    "            input, target = feature.cuda(), label.cuda()\n",
    "\n",
    "        regular_lr = cosine_lr.get_regular_lr(iter_num)\n",
    "        cosine_lr._set_lr(optimizer, regular_lr)\n",
    "\n",
    "        prediction = model(input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pixel_loss = loss(prediction, target)\n",
    "        epoch_loss += pixel_loss.item()\n",
    "        pixel_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "        if iter_num % print_freq == 0 or iter_num == 100:\n",
    "            break\n",
    "\n",
    "    with open(log_file, 'a') as logs:\n",
    "        logs.write(\"===> Iters({}/{}): Loss: {:.4f}\".format(iter_num, model_config['max_iters'], epoch_loss / print_freq) + \"\\n\")\n",
    "\n",
    "    oneValue = epoch_loss / print_freq\n",
    "    lossList.append(oneValue)\n",
    "    if len(lossList) > 10:\n",
    "        lossList.pop(0)\n",
    "        sumValue = sum(lossList)\n",
    "\n",
    "    if iter_num % Show_freq == 0 or iter_num == 100:\n",
    "        print(\"===> Iters({}/{}): Loss: {:.4f}\".format(iter_num, model_config['max_iters'], epoch_loss / print_freq) + \"\\n\")\n",
    "\n",
    "    if iter_num % Show_freq == 0 or iter_num == 100:\n",
    "        later = datetime.now()\n",
    "        difference = (later - now).total_seconds()\n",
    "        difference_minutes = int(difference // 60)\n",
    "        difference_seconds = int(difference % 60)\n",
    "        max_iters = model_config['max_iters']\n",
    "        expect_difference = difference * (max_iters / iter_num)\n",
    "        difference_minutes = int(expect_difference // 60)\n",
    "        difference_seconds = int(expect_difference % 60)\n",
    "\n",
    "    epoch_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    'task': 'Congestion',\n",
    "    'save_path': './Congestion/',\n",
    "    'pretrained': None,\n",
    "    'max_iters': 200000,\n",
    "    'plot_roc': False,\n",
    "    'arg_file': None,\n",
    "    'cpu': False,\n",
    "    'dataroot': '/home/palaniappan-r/Repos/boosted-UNet/training_datasets/congestion_train_set/congestion',\n",
    "    'ann_file_train': './files/train.csv',\n",
    "    'ann_file_test': './files/test.csv',\n",
    "    'dataset_type': 'CongestionDataset',\n",
    "    'batch_size': 4,\n",
    "    'aug_pipeline': ['Flip'],\n",
    "    'model_type': 'Congestion_Prediction_Net',\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 1,\n",
    "    'lr': 0.0002,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss_type': 'MSELoss',\n",
    "    'eval_metric': ['NRMS', 'SSIM'],\n",
    "    'ann_file': './files/test.csv',\n",
    "    'test_mode': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Congestion_Prediction_Net(\n",
       "  (uNet): RouteNet(\n",
       "    (encoder): Encoder(\n",
       "      (c1): conv(\n",
       "        (main): Sequential(\n",
       "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (c2): conv(\n",
       "        (main): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (c3): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv1): conv(\n",
       "        (main): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (upc1): upconv(\n",
       "        (main): Sequential(\n",
       "          (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): conv(\n",
       "        (main): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (upc2): upconv(\n",
       "        (main): Sequential(\n",
       "          (0): ConvTranspose2d(48, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = build_dataset(test_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 CircuitNet. All rights reserved.\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.metrics import build_metric\n",
    "\n",
    "\n",
    "def inference(test_config, model):\n",
    "    # Build metrics\n",
    "    metrics = {k:build_metric(k) for k in test_config['eval_metric']}\n",
    "    avg_metrics = {k:0 for k in test_config['eval_metric']}\n",
    "\n",
    "    count =0\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for feature, label, label_path in dataset:\n",
    "            if test_config['cpu']:\n",
    "                input, target = feature, label\n",
    "            else:\n",
    "                input, target = feature.cuda(), label.cuda()\n",
    "\n",
    "            prediction = model(input)\n",
    "            for metric, metric_func in metrics.items():\n",
    "                if not metric_func(target.cpu(), prediction.squeeze(1).cpu()) == 1:\n",
    "                    avg_metrics[metric] += metric_func(target.cpu(), prediction.squeeze(1).cpu())\n",
    "\n",
    "            if test_config['plot_roc']:\n",
    "                save_path = osp.join(test_config['save_path'], 'test_result')\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                file_name = osp.splitext(osp.basename(label_path[0]))[0]\n",
    "                save_path = osp.join(save_path, f'{file_name}.npy')\n",
    "                output_final = prediction.float().detach().cpu().numpy()\n",
    "                np.save(save_path, output_final)\n",
    "                count +=1\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "\n",
    "    for metric, avg_metric in avg_metrics.items():\n",
    "        print(\"===> Avg. {}: {:.4f}\".format(metric, avg_metric / len(dataset))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3164/3164 [01:03<00:00, 50.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Avg. NRMS: 0.0460\n",
      "===> Avg. SSIM: 0.7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference(test_config, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RoutingCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
